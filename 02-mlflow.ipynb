{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import matplotlib.pyplot as plt                   # For charts and visualizations\n",
    "from IPython.display import Image                 # For displaying images in the notebook\n",
    "from IPython.display import display               # For displaying outputs in the notebook\n",
    "from time import gmtime, strftime                 # For labeling SageMaker models, endpoints, etc.\n",
    "import sys                                        # For writing outputs to notebook\n",
    "import math                                       # For ceiling function\n",
    "import json                                       # For parsing hosting outputs\n",
    "import os                                         # For manipulating filepath names\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket =sess.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "# Define IAM role\n",
    "sagemaker_role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./bank-data/bank-additional-full.csv')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 20)         # Keep the output on one page\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source = sess.upload_data('./bank-data/bank-additional-full.csv', bucket=bucket, key_prefix=f'{prefix}/input_data')\n",
    "input_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import mlflow\n",
    "\n",
    "def _parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\n",
    "    parser.add_argument('--filepath', type=str, default='/opt/ml/processing/input/')\n",
    "    parser.add_argument('--filename', type=str, default='bank-additional-full.csv')\n",
    "    parser.add_argument('--outputpath', type=str, default='/opt/ml/processing/output/')\n",
    "    parser.add_argument('--categorical_features', type=str, default='y, job, marital, education, default, housing, loan, contact, month, day_of_week, poutcome')\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Process arguments\n",
    "    args, _ = _parse_args()\n",
    "\n",
    "    mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_ARN'])\n",
    "    print(f\"logging experiment to {os.environ['MLFLOW_TRACKING_ARN']}\")\n",
    "    mlflow.set_experiment(os.environ['EXPERIMENT_NAME'])        \n",
    "\n",
    "    with mlflow.start_run(run_name=f\"Preprocessing\") as run:            \n",
    "        \n",
    "        mlflow.autolog()\n",
    "\n",
    "        # Load data\n",
    "        df = pd.read_csv(os.path.join(args.filepath, args.filename))\n",
    "\n",
    "        mlflow.log_input(\n",
    "            mlflow.data.from_pandas(df, args.filepath, targets='y'),\n",
    "            context=\"InputData\",\n",
    "        )\n",
    "       \n",
    "        # Change the value . into _\n",
    "        df = df.replace(regex=r'\\.', value='_')\n",
    "        df = df.replace(regex=r'\\_$', value='')\n",
    "        # Add two new indicators\n",
    "        df[\"no_previous_contact\"] = (df[\"pdays\"] == 999).astype(int)\n",
    "        df[\"not_working\"] = df[\"job\"].isin([\"student\", \"retired\", \"unemployed\"]).astype(int)\n",
    "        df = df.drop(['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'], axis=1)\n",
    "        # Encode the categorical features\n",
    "        df = pd.get_dummies(df)\n",
    "        # Train, test, validation split\n",
    "        train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=42), [int(0.7 * len(df)), int(0.9 * len(df))])   # Randomly sort the data then split out first 70%, second 20%, and last 10%\n",
    "\n",
    "        mlflow.log_param(\"train_split\", 0.7)\n",
    "        mlflow.log_param(\"validation_split\", 0.2)\n",
    "        mlflow.log_param(\"test_split\", 0.1)\n",
    "\n",
    "        mlflow.log_input(\n",
    "            mlflow.data.from_pandas(train_data, args.outputpath),\n",
    "            context=\"TrainingData\",\n",
    "        )\n",
    "        mlflow.log_input(\n",
    "            mlflow.data.from_pandas(validation_data, args.outputpath),\n",
    "            context=\"ValidationData\",\n",
    "        )\n",
    "        mlflow.log_input(\n",
    "            mlflow.data.from_pandas(test_data, args.outputpath),\n",
    "            context=\"TestData\",\n",
    "        )\n",
    "\n",
    "        # Local store\n",
    "        pd.concat([train_data['y_yes'], train_data.drop(['y_yes','y_no'], axis=1)], axis=1).to_csv(os.path.join(args.outputpath, 'train/train.csv'), index=False, header=False)\n",
    "        pd.concat([validation_data['y_yes'], validation_data.drop(['y_yes','y_no'], axis=1)], axis=1).to_csv(os.path.join(args.outputpath, 'validation/validation.csv'), index=False, header=False)\n",
    "        test_data['y_yes'].to_csv(os.path.join(args.outputpath, 'test/test_y.csv'), index=False, header=False)\n",
    "        test_data.drop(['y_yes','y_no'], axis=1).to_csv(os.path.join(args.outputpath, 'test/test_x.csv'), index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/requirements.txt\n",
    "mlflow\n",
    "sagemaker-mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the MLflow tracking server ARN and pass it to the processing job below\n",
    "import boto3\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.sklearn import SKLearn\n",
    "\n",
    "train_path = f\"s3://{bucket}/{prefix}/train\"\n",
    "validation_path = f\"s3://{bucket}/{prefix}/validation\"\n",
    "test_path = f\"s3://{bucket}/{prefix}/test\"\n",
    "\n",
    "sklearn_processor = FrameworkProcessor(\n",
    "    estimator_cls = SKLearn,\n",
    "    framework_version=\"1.2-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1, \n",
    "    base_job_name='sm-immday-skprocessing',\n",
    "    env={\"MLFLOW_TRACKING_ARN\": mlflow_arn, \"EXPERIMENT_NAME\": \"bank-marketing\"}\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code='preprocessing.py',\n",
    "    source_dir='./src',\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_source, destination=\"/opt/ml/processing/input\", s3_input_mode=\"File\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", source=\"/opt/ml/processing/output/train\", destination=train_path),\n",
    "        ProcessingOutput(output_name=\"validation_data\", source=\"/opt/ml/processing/output/validation\", destination=validation_path),\n",
    "        ProcessingOutput(output_name=\"test_data\", source=\"/opt/ml/processing/output/test\", destination=test_path),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we know that most of our features have skewed distributions, some are highly correlated with one another, and some appear to have non-linear relationships with our target variable.  Also, for targeting future prospects, good predictive accuracy is preferred to being able to explain why that prospect was targeted.  Taken together, these aspects make gradient boosted trees a good candidate algorithm.\n",
    "\n",
    "There are several intricacies to understanding the algorithm, but at a high level, gradient boosted trees works by combining predictions from many simple models, each of which tries to address the weaknesses of the previous models.  By doing this the collection of simple models can actually outperform large, complex models.  Other Amazon SageMaker notebooks elaborate on gradient boosting trees further and how they differ from similar algorithms.\n",
    "\n",
    "`xgboost` is an extremely popular, open-source package for gradient boosted trees.  It is computationally powerful, fully featured, and has been successfully used in many machine learning competitions.  Let's start with a simple `xgboost` model, trained using Amazon SageMaker's managed, distributed training framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3, which also specify that the content type is CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_path.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=validation_path.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll need to specify training parameters to the estimator.  This includes:\n",
    "1. The `xgboost` algorithm container\n",
    "1. The IAM role to use\n",
    "1. Training instance type and count\n",
    "1. S3 location for output data\n",
    "1. Algorithm hyperparameters\n",
    "\n",
    "And then a `.fit()` function which specifies:\n",
    "1. S3 location for output data.  In this case we have both a training and validation set which are passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/training.py\n",
    "# Training algorithm\n",
    "import os\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,precision_score,recall_score\n",
    "\n",
    " \n",
    "def _parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # XGBoost hyperparameters\n",
    "    parser.add_argument('--eta', type=float, default=0.2, help='Learning rate (default: 0.2)')\n",
    "    parser.add_argument('--gamma', type=float, default=4, help='Minimum loss reduction required to make a further partition (default: 4)')\n",
    "    parser.add_argument('--min_child_weight', type=int, default=6, help='Minimum sum of instance weight needed in a child (default: 6)')\n",
    "    parser.add_argument('--subsample', type=float, default=0.8, help='Subsample ratio of training instances (default: 0.8)')\n",
    "    parser.add_argument('--silent', type=int, default=0, help='Logging mode - quiet means silent mode (default: 0)')\n",
    "    parser.add_argument('--objective', type=str, default='binary:logistic', help='Learning task objective (default: binary:logistic)')\n",
    "    parser.add_argument('--num_round', type=int, default=100, help='Number of boosting rounds/trees (default: 100)')\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--validation', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "    \n",
    "    return parser.parse_known_args()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Process arguments\n",
    "    args, _ = _parse_args()\n",
    "\n",
    "    train_data = pd.read_csv(os.path.join(args.train, 'train.csv'), header=None)\n",
    "    validation_data = pd.read_csv(os.path.join(args.validation, 'validation.csv'), header=None)\n",
    "    \n",
    "    # labels are in the first column\n",
    "    train_y = train_data.iloc[:, 0]\n",
    "    train_X = train_data.iloc[:, 1:]\n",
    "    validation_y = validation_data.iloc[:, 0]\n",
    "    validation_X = validation_data.iloc[:, 1:]\n",
    "\n",
    "    mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_ARN'])\n",
    "    print(f\"logging experiment to {os.environ['MLFLOW_TRACKING_ARN']}\")\n",
    "    mlflow.set_experiment(os.environ['EXPERIMENT_NAME'])\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"Training\") as run:               \n",
    "        mlflow.autolog()\n",
    "             \n",
    "        # Creating DMatrix(es)\n",
    "        dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "        dval = xgboost.DMatrix(validation_X, label=validation_y)\n",
    "        watchlist = [(dtrain, \"train\"), (dval, \"validation\")]\n",
    "\n",
    "        param_dist = {\n",
    "            \"eta\": args.eta,\n",
    "            \"gamma\": args.gamma,\n",
    "            \"min_child_weight\": args.min_child_weight,\n",
    "            \"subsample\": args.subsample,\n",
    "            \"silent\": args.silent,\n",
    "            \"objective\": str(args.objective),\n",
    "            \"num_round\": args.num_round\n",
    "        }        \n",
    "    \n",
    "        xgb = xgboost.train(\n",
    "            params=param_dist,\n",
    "            dtrain=dtrain,\n",
    "            evals=watchlist,\n",
    "            num_boost_round=args.num_round)\n",
    "    \n",
    "        predictions = xgb.predict(dval)\n",
    "    \n",
    "        print (pd.crosstab(index=validation_y, columns=np.round(predictions),\n",
    "                           rownames=['Actuals'], colnames=['Predictions'], margins=True))\n",
    "        \n",
    "        rounded_predict = np.round(predictions)\n",
    "    \n",
    "        val_accuracy = accuracy_score(validation_y, rounded_predict)\n",
    "        val_precision = precision_score(validation_y, rounded_predict)\n",
    "        val_recall = recall_score(validation_y, rounded_predict)\n",
    "    \n",
    "        print(\"Accuracy Model A: %.2f%%\" % (val_accuracy * 100.0))            \n",
    "        print(\"Precision Model A: %.2f\" % (val_precision))\n",
    "        print(\"Recall Model A: %.2f\" % (val_recall))\n",
    "        \n",
    "        # Log additional metrics, next to the default ones logged automatically\n",
    "        mlflow.log_metric(\"Accuracy Model A\", val_accuracy * 100.0)\n",
    "        mlflow.log_metric(\"Precision Model A\", val_precision)\n",
    "        mlflow.log_metric(\"Recall Model A\", val_recall)\n",
    "            \n",
    "        val_auc = roc_auc_score(validation_y, predictions)\n",
    "        \n",
    "        print(\"Validation AUC A: %.2f\" % (val_auc))\n",
    "        mlflow.log_metric(\"Validation AUC A\", val_auc)\n",
    "    \n",
    "        model_file_path=\"/opt/ml/model/xgboost-model\"\n",
    "        os.makedirs(os.path.dirname(model_file_path), exist_ok=True)\n",
    "        \n",
    "        pkl.dump(xgb, open(model_file_path, 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/requirements.txt\n",
    "mlflow==2.17.0\n",
    "sagemaker-mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "xgb = XGBoost(\n",
    "    entry_point='training.py',\n",
    "    source_dir='./src',\n",
    "    framework_version='1.7-1',\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    environment={\"MLFLOW_TRACKING_ARN\": mlflow_arn, \"EXPERIMENT_NAME\": \"bank-marketing\"},\n",
    "    keep_alive_period_in_seconds=3600)\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    silent=0,\n",
    "    num_round=100\n",
    ")\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = xgb.output_path\n",
    "training_job_name = xgb.latest_training_job.name\n",
    "\n",
    "%store model_output_path\n",
    "%store training_job_name"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "sagemaker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
