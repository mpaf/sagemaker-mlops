{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using @remote Decorator and ModelBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $train_path/train.csv /tmp/train.csv\n",
    "!aws s3 cp $validation_path/validation.csv /tmp/val.csv\n",
    "\n",
    "train = pd.read_csv('/tmp/train.csv')\n",
    "val = pd.read_csv('/tmp/val.csv')\n",
    "\n",
    "train_x = train.iloc[:, 1:]\n",
    "train_y = train.iloc[:, 0]\n",
    "\n",
    "val_x = val.iloc[:, 1:]\n",
    "val_y = val.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will use XGBoost to train a logistic regression model using the preprocessed data generated in the previous step. Again, you will use a standard Python function that accepts some of the XGBoost hyperparameters as input and returns the model.\n",
    "\n",
    "The following cell annotates the training function with the @remote decorator to run the Python function as a SageMaker job without requiring any other modifications to the function code. Feel free to comment out the remote decorator in the cells below to seamlesssly move from running the function remotely via SageMaker Training to local execution. If you comment out the decorator to run the function locally, you will need to run this command in the terminal to give permission to the output directory where the function will save the models: sudo chmod -R 777 /opt/ml/model. You don't need to run this command if you leave the remote decorator in, since the config.yaml file runs that command before executing the training job.\n",
    "\n",
    "Running the training function will initiate a SageMaker training job because the function is decoarated with the @remote decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mkdir /opt/ml/model\n",
    "!sudo chmod -R 777 /opt/ml/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import pickle as pkl\n",
    "import xgboost\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import xgboost\n",
    "from sagemaker.remote_function import remote\n",
    "\n",
    "\n",
    "#@remote(instance_type='ml.m5.xlarge', job_name_prefix=f\"decorator-train\")\n",
    "def train(X_train, y_train, X_val, y_val, experiment_name, tracking_server_arn,\n",
    "          eta=0.15, \n",
    "          max_depth=3, \n",
    "          gamma=0.0,\n",
    "          min_child_weight=1,\n",
    "          verbosity=0,\n",
    "          objective='binary:logistic',\n",
    "          eval_metric='auc',\n",
    "          num_boost_round=50):\n",
    "\n",
    "    print('Train features shape: {}'.format(X_train.shape))\n",
    "    print('Train labels shape: {}'.format(y_train.shape))\n",
    "    print('Validation features shape: {}'.format(X_val.shape))\n",
    "    print('Validation labels shape: {}'.format(y_val.shape))        \n",
    "    \n",
    "    mlflow.set_tracking_uri(tracking_server_arn)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"Training\") as run:               \n",
    "        mlflow.autolog()\n",
    "             \n",
    "        # Creating DMatrix(es)\n",
    "        dtrain = xgboost.DMatrix(X_train.values, label=y_train)\n",
    "        dval = xgboost.DMatrix(X_val.values, label=y_val)\n",
    "        watchlist = [(dtrain, \"train\"), (dval, \"validation\")]\n",
    "    \n",
    "        print('')\n",
    "        print (f'===Starting training with max_depth {max_depth}===')\n",
    "        \n",
    "        param_dist = {\n",
    "            \"max_depth\": max_depth,\n",
    "            \"eta\": eta,\n",
    "            \"gamma\": gamma,\n",
    "            \"min_child_weight\": min_child_weight,\n",
    "            \"verbosity\": verbosity,\n",
    "            \"objective\": objective,\n",
    "            \"eval_metric\": eval_metric\n",
    "        }        \n",
    "    \n",
    "        xgb = xgboost.train(\n",
    "            params=param_dist,\n",
    "            dtrain=dtrain,\n",
    "            evals=watchlist,\n",
    "            num_boost_round=num_boost_round)\n",
    "    \n",
    "        predictions = xgb.predict(dval)\n",
    "    \n",
    "        print (\"Metrics for validation set\")\n",
    "        print('')\n",
    "        print (pd.crosstab(index=y_val, columns=np.round(predictions),\n",
    "                           rownames=['Actuals'], colnames=['Predictions'], margins=True))\n",
    "        \n",
    "        rounded_predict = np.round(predictions)\n",
    "    \n",
    "        val_accuracy = accuracy_score(y_val, rounded_predict)\n",
    "        val_precision = precision_score(y_val, rounded_predict)\n",
    "        val_recall = recall_score(y_val, rounded_predict)\n",
    "    \n",
    "        print(\"Accuracy Model A: %.2f%%\" % (val_accuracy * 100.0))            \n",
    "        print(\"Precision Model A: %.2f\" % (val_precision))\n",
    "        print(\"Recall Model A: %.2f\" % (val_recall))\n",
    "        \n",
    "        # Log additional metrics, next to the default ones logged automatically\n",
    "        mlflow.log_metric(\"Accuracy Model A\", val_accuracy * 100.0)\n",
    "        mlflow.log_metric(\"Precision Model A\", val_precision)\n",
    "        mlflow.log_metric(\"Recall Model A\", val_recall)\n",
    "        \n",
    "        from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "        val_auc = roc_auc_score(y_val, predictions)\n",
    "        \n",
    "        print(\"Validation AUC A: %.2f\" % (val_auc))\n",
    "        mlflow.log_metric(\"Validation AUC A\", val_auc)\n",
    "    \n",
    "        model_file_path=\"/opt/ml/model/xgboost-model\"\n",
    "        os.makedirs(os.path.dirname(model_file_path), exist_ok=True)\n",
    "        xgb.save_model(model_file_path)\n",
    "\n",
    "    return xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'new-bank-experiment'\n",
    "\n",
    "booster = train(train_x, train_y, val_x, val_y, experiment_name, tracking_server_arn)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "sagemaker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
