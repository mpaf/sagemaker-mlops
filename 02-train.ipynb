{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we know that most of our features have skewed distributions, some are highly correlated with one another, and some appear to have non-linear relationships with our target variable.  Also, for targeting future prospects, good predictive accuracy is preferred to being able to explain why that prospect was targeted.  Taken together, these aspects make gradient boosted trees a good candidate algorithm.\n",
    "\n",
    "There are several intricacies to understanding the algorithm, but at a high level, gradient boosted trees works by combining predictions from many simple models, each of which tries to address the weaknesses of the previous models.  By doing this the collection of simple models can actually outperform large, complex models.  Other Amazon SageMaker notebooks elaborate on gradient boosting trees further and how they differ from similar algorithms.\n",
    "\n",
    "`xgboost` is an extremely popular, open-source package for gradient boosted trees.  It is computationally powerful, fully featured, and has been successfully used in many machine learning competitions.  Let's start with a simple `xgboost` model, trained using Amazon SageMaker's managed, distributed training framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3, which also specify that the content type is CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_path.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=validation_path.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll need to specify training parameters to the estimator.  This includes:\n",
    "1. The `xgboost` algorithm container\n",
    "1. The IAM role to use\n",
    "1. Training instance type and count\n",
    "1. S3 location for output data\n",
    "1. Algorithm hyperparameters\n",
    "\n",
    "And then a `.fit()` function which specifies:\n",
    "1. S3 location for output data.  In this case we have both a training and validation set which are passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/training.py\n",
    "# Training algorithm\n",
    "import os\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import argparse\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,precision_score,recall_score\n",
    "\n",
    " \n",
    "def _parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # XGBoost hyperparameters\n",
    "    parser.add_argument('--eta', type=float, default=0.2, help='Learning rate (default: 0.2)')\n",
    "    parser.add_argument('--gamma', type=float, default=4, help='Minimum loss reduction required to make a further partition (default: 4)')\n",
    "    parser.add_argument('--min_child_weight', type=int, default=6, help='Minimum sum of instance weight needed in a child (default: 6)')\n",
    "    parser.add_argument('--subsample', type=float, default=0.8, help='Subsample ratio of training instances (default: 0.8)')\n",
    "    parser.add_argument('--silent', type=int, default=0, help='Logging mode - quiet means silent mode (default: 0)')\n",
    "    parser.add_argument('--objective', type=str, default='binary:logistic', help='Learning task objective (default: binary:logistic)')\n",
    "    parser.add_argument('--num_round', type=int, default=100, help='Number of boosting rounds/trees (default: 100)')\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--validation', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "    \n",
    "    return parser.parse_known_args()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Process arguments\n",
    "    args, _ = _parse_args()\n",
    "\n",
    "    train_data = pd.read_csv(os.path.join(args.train, 'train.csv'), header=None)\n",
    "    validation_data = pd.read_csv(os.path.join(args.validation, 'validation.csv'), header=None)\n",
    "    \n",
    "    # labels are in the first column\n",
    "    train_y = train_data.iloc[:, 0]\n",
    "    train_X = train_data.iloc[:, 1:]\n",
    "    validation_y = validation_data.iloc[:, 0]\n",
    "    validation_X = validation_data.iloc[:, 1:]\n",
    "\n",
    "    mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_ARN'])\n",
    "    print(f\"logging experiment to {os.environ['MLFLOW_TRACKING_ARN']}\")\n",
    "    mlflow.set_experiment(os.environ['EXPERIMENT_NAME'])\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"Training\") as run:               \n",
    "        mlflow.autolog()\n",
    "             \n",
    "        # Creating DMatrix(es)\n",
    "        dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "        dval = xgboost.DMatrix(validation_X, label=validation_y)\n",
    "        watchlist = [(dtrain, \"train\"), (dval, \"validation\")]\n",
    "\n",
    "        param_dist = {\n",
    "            \"eta\": args.eta,\n",
    "            \"gamma\": args.gamma,\n",
    "            \"min_child_weight\": args.min_child_weight,\n",
    "            \"subsample\": args.subsample,\n",
    "            \"silent\": args.silent,\n",
    "            \"objective\": str(args.objective),\n",
    "            \"num_round\": args.num_round\n",
    "        }        \n",
    "    \n",
    "        xgb = xgboost.train(\n",
    "            params=param_dist,\n",
    "            dtrain=dtrain,\n",
    "            evals=watchlist,\n",
    "            num_boost_round=args.num_round)\n",
    "    \n",
    "        predictions = xgb.predict(dval)\n",
    "    \n",
    "        print (pd.crosstab(index=validation_y, columns=np.round(predictions),\n",
    "                           rownames=['Actuals'], colnames=['Predictions'], margins=True))\n",
    "        \n",
    "        rounded_predict = np.round(predictions)\n",
    "    \n",
    "        val_accuracy = accuracy_score(validation_y, rounded_predict)\n",
    "        val_precision = precision_score(validation_y, rounded_predict)\n",
    "        val_recall = recall_score(validation_y, rounded_predict)\n",
    "    \n",
    "        print(\"Accuracy Model A: %.2f%%\" % (val_accuracy * 100.0))            \n",
    "        print(\"Precision Model A: %.2f\" % (val_precision))\n",
    "        print(\"Recall Model A: %.2f\" % (val_recall))\n",
    "        \n",
    "        # Log additional metrics, next to the default ones logged automatically\n",
    "        mlflow.log_metric(\"Accuracy Model A\", val_accuracy * 100.0)\n",
    "        mlflow.log_metric(\"Precision Model A\", val_precision)\n",
    "        mlflow.log_metric(\"Recall Model A\", val_recall)\n",
    "            \n",
    "        val_auc = roc_auc_score(validation_y, predictions)\n",
    "        \n",
    "        print(\"Validation AUC A: %.2f\" % (val_auc))\n",
    "        mlflow.log_metric(\"Validation AUC A\", val_auc)\n",
    "    \n",
    "        model_file_path=\"/opt/ml/model/xgboost_model.bin\"\n",
    "        os.makedirs(os.path.dirname(model_file_path), exist_ok=True)\n",
    "        xgb.save_model(model_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/requirements.txt\n",
    "mlflow==2.17.0\n",
    "sagemaker-mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "xgb = XGBoost(\n",
    "    entry_point='training.py',\n",
    "    source_dir='./src',\n",
    "    framework_version='1.7-1',\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    environment={\"MLFLOW_TRACKING_ARN\": tracking_server_arn, \"EXPERIMENT_NAME\": \"bank-marketing\"},\n",
    "    keep_alive_period_in_seconds=3600)\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    silent=0,\n",
    "    num_round=100\n",
    ")\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "sagemaker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
